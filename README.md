# DramaSR-LRM: 

## Environment Setup

Ensure you have package environments capable of running LLaMA-Factory and verl-tool frameworks.

## Data Preparation

We provide data for 10 Chinese dramas and 3 English dramas, located in the `LRM/<drama_name>` folder.

You can use the methods described in this section to generate inference data via API, and then split it into data for SFT and RL training.

### Data Generation

This step enables generating inference data using APIs.

Navigate to `DramaSR-LRM/LRM` directory, run `tool_master.py`, and specify `--series_name` as the drama name and `--end_name` as the suffix for output files. You need to fill in your api_key and base_url in `chat.py`.

The API-generated inference data will be stored in the `test_result` folder.

### Data Split

This step enables extracting SFT and RL data from the inference data according to specified ratios.

Navigate to `DramaSR-LRM/training` directory, run `data_split.py`, and specify `--series_name` and `--end_name` parameters to match those used in the previous Data Generation step. The program will load required data from `DramaSR-LRM/LRM/test_result`. Other parameters that need to be specified are explained in the `data_split.py` file. You can customize your own data-split file to adjust the ratio scheme.

The generated data allocation results from this step will be stored in the current `training` directory, containing split_data and meta_data.

### Generate SFT Data

This step enables processing the SFT data from the allocation results into the required format for training.

Navigate to `DramaSR-LRM/training` directory, run `generate_sft_data.py`, and specify `--input_filename` as the split_data file generated by Data Split, and `--dataset_format` as `alpaca` (other formats are currently not supported).

The generated SFT data from this step will be stored in the current `training` directory.

**The project provides pre-generated SFT data in the `training` directory: `tool_master_ren_shi_jian_split_data_2026-01-21_alpaca.json`**

### Generate RL Data

This step enables processing the RL data from the allocation results into the required format for training.

Navigate to `DramaSR-LRM/training` directory, run `generate_rl_data.py`, and specify `--input_filename` as the split_data file generated by Data Split, `--dataset_format` as `verl` (other formats are currently not supported), `--series_name` as the drama name, and `--val_ratio` as the test set ratio (set to 0.1 in the project).

The generated RL data (divided into train and val files) from this step will be stored in the current `training` directory.

**The project provides pre-generated RL data in the `training` directory: `tool_master_zhen_huan_zhuan_split_data_2026-01-22_verl_0.1_train.parquet` and `tool_master_zhen_huan_zhuan_split_data_2026-01-22_verl_0.1_val.parquet`**

## Training

### SFT Training

We use the LLaMA-Factory framework for SFT training. The framework is located at `DramaSR-LRM/LLaMA-Factory`.

We have customized two aspects of the framework:
1. `DramaSR-LRM/LLaMA-Factory/examples/train_full/speaker_rec.yaml` - Training parameter configuration file
2. `DramaSR-LRM/LLaMA-Factory/data/dataset_info.json` - Training data information index file

You need to copy the SFT data to `DramaSR-LRM/LLaMA-Factory/data` directory and define new datasets in `dataset_info.json`. The project provides a built-in dataset `speaker_rec`, corresponding to the SFT data provided with the project. When creating new SFT datasets, compared to `speaker_rec`, you only need to modify the `file_name` parameter to point to your SFT file; other parameters need not be adjusted.

You need to define the base model path, dataset name (corresponding to the key in `dataset_info.json`), and model save path in `speaker_rec.yaml`. You can also customize other parameters.

After completing the above steps, you should navigate to the `DramaSR-LRM/LLaMA-Factory` directory and execute `llamafactory-cli train ./examples/train_full/speaker_rec.yaml` to start training.

### RL Training

We use the verl-tool framework for RL training. The framework is located at `DramaSR-LRM/verl-tool`.

We have customized three aspects of the framework:
1. `DramaSR-LRM/verl-tool/examples/train/speaker_rec.sh` - Training parameter configuration file
2. `DramaSR-LRM/verl-tool/verl_tool/servers/tools/speaker_rec_tool.py` - Tool invocation logic file
3. `DramaSR-LRM/verl-tool/verl/verl/utils/reward_score/__init__.py` - Reward function design file

You need to define the base model path, training data file path, test data file path, model save path, training rollout save path, and test rollout save path in `speaker_rec.sh`. You can also customize other parameters. Currently, the project only supports RL training with Chinese dramas.

After completing the above steps, you should navigate to the `DramaSR-LRM/verl-tool` directory and execute `bash ./examples/train/speaker_rec.sh` to start training.

## Evaluate

Finally, we can use the trained models to test speaker recognition performance on dramas.

Navigate to `DramaSR-LRM/LRM` directory, define the model path in `model_batch.sh`, and run the `model_batch.sh` script. You can adjust GPU quantity configuration, base port, and other parameters according to environment requirements. After modification, please check `model_chat.py` to see if parameter changes need to be synchronized. The script loads one model instance per GPU by default.

Run `tool_master.py`, and specify `--series_name` as the drama name, `--end_name` as the suffix for output files, `--if_api` as `no`, and `--port_s` and `--port_e` as the port offset range available for program interaction, which is a closed interval. For example, specifying `--port_s` as 1 and `--port_e` as 1 means the program only interacts with port `<baseport+1>`. The program supports multi-threaded concurrent access, so deploying multiple models on multiple ports for interaction with the program can significantly improve evaluation speed.

The generated evaluation initial text will be stored in the `test_result` folder.

You can run `data_collect.py` to collect and analyze the results. When running, specify `--series_name` as the drama name and `--end_name` as the same suffix used during evaluation.

The collection and analysis results will be stored in `test_result/statistic_count`.

**Our models are located at: `DramaSR-LRM/DramaSR-LRM`**  
**Our evaluation results are located at: `DramaSR-LRM/LRM/experiment_result`**
